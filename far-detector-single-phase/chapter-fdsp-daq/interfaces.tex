
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Interfaces (David Cussans \& Matt Graham)}
\label{sec:fdsp-daq-intfc}

\metainfo{5 Pages}

\fixme{Include an image of each interface in appropriate section.  Can maybe refer to Fig.~\ref{fig:daq-overview} but it currently lacks some of the interfaces.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{TPC Electronics}
\label{sec:fdsp-daq-intfc-elec}

The interface between the Single Phase TPC detectors and the DAQ is described in Dune-DocDB-6742.

Data from the cold electronics front end motherboards are 8b10 encoded
and sent to the Warm Interface boards (WIBs) on copper cables at a bit
rate of 1.28GBit/s. There are two options being considered for the
WIBs. In one the data are simply converted to optical signals and
transmitted to the DAQ in the CUC on 1.28GBit/s optical links with a
total of 80 fibres per APA. In the second option the WIBs aggregate
the data onto links running at ~ 10GBit/s before transmission to the
DAQ with a total of ten fibres per APA. In both cases the data are
received on rear transition modules connected to the COB ATCA boards
that receive the data ( see section~{sec:fdsp-daq-ltr} ).



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{PD Electronics}
\label{sec:fdsp-daq-intfc-photon}

The interface between the Single Phase Photon Detection System(PDS)
and the DAQ is described in DUNE-doc-6727. The signal to noise ratio
of the SiPM signals is high enough to allow the data to be safely
zero-suppressed. This reduces the data flow so that a bandwidth of
8GBit/s per APA is sufficent to transfer the zero supressed data to
the DAQ, with an order of magnitude safety factor. The link from the
single phase cryostats to the CUC will be implemented as either eight
1000Base-SX links  or a single 10GBase-LR link per APA.

The data on the links will be encoded using UDP/IP to allow easy
reception by the DUNE DAQ server PCs.

There is a single physical pathway between the PDS and the DAQ for
both data and information used for data selection. The first stage of
the DAQ selects trigger primitives from this data stream and forwards
them to the data selection system.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Offline Computing (Kurt Biery)}
\label{sec:fdsp-daq-intfc-fnal-cmptg}

The interface between the \dword{daq} and Offline Computing is described in
DUNE-doc-7123. The \dword{daq} team will be responsible for reducing the
data volume to the level that is agreed upon by all interested parties,
and the raw data files will be transfered from SURF to Fermilab using
a dedicated network connection. A disk buffer will be provided by the
\dword{daq} on or near the SURF site to hold the data from several days
of data taking so that the operation of the experiment will not be affected
if there happens to be a network disruption between SURF and Fermilab.

During stable running, the data volume that will be produced by the
\dword{daq} systems of all four \dwords{detmodule} will be no larger
than 30 PB/yr. This rate is expected to be independent of the number
of \dwords{detmodule} that are operational. During the construction
of the second, third, and fourth \dwords{detmodule}, the extra rate
per \dword{detmodule} will be used to gather data to aid in the
refinement of the data selection algorithms.
During commissioning, the data rate is expected to  be higher,
and it is anticipated that a data volume
of 30 PB will be necessary to commission a \dword{detmodule}.

The disk buffer at SURF is planned to be 300 TB in size.  The data
link from SURF to Fermilab will support 20 Gbps.  The Offline Computing
team will be responsible for developing the software to manage the
transfer of files from SURF to Fermilab. The \dword{daq} team will be
responsible for producing a
reference implementation of the software that is used to access and
decode the raw electronics data. They will also be responsible for
providing the framework for real-time \dword{dqm}, and
both groups will have responsibility for developing the \dword{dqm}
algorithms that run inside this framework.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Slow Control}
\label{sec:fdsp-daq-intfc-ext}

The Cryogenics and Slow Controls (CISC) systems monitor detector
hardware and conditions not directly involved in taking the data
described above.  That data will be stored both locally (in CISC
database servers in the CUC) and offline (the databases will be
replicated back to Fermilab) in a relational database indexed by
timestamp.  This will allow bi-directional communications between DAQ
and CISC by reading or inserting data into the database as needed for
non time-critical information.  

For prompt, time sensitive status information such as ``Run is in
progress'' or ``Camera is on'', a low-latency software status register
will be available on the local network to both systems.

There is not a hardware interface, aside from the fact that several
racks of CISC servers will be in the DAQ room in the CUC, and that rack
monitors in DAQ racks will be read out into the CISC data stream.

Note that life and hardware safety critical items will be hardware
interlocked in the standard Fermilab fashion, and fall outside of this
interface's scope.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{External Systems (Giles \& Alec)}
\label{sec:fdsp-daq-intfc-ext}

\fixme{Need to receive information on beam spills (Giles) , SNEWS (Alec).}

The DAQ will need to save data based on external triggers, for example:
for times around when the pulse of neutrinos from LBNF arrives at the
Far Detector; or if notice of an interesting astrophysical event is
given by SNEWS\cite{snews} or LIGO.  This could involve going back in
time to save data that has already been buffered (see
Sec.~\ref{sec:fdsp-daq-ltr}), or changing the trigger or zero
suppression criteria for data in the interesting time period.

\paragraph{Beam Trigger:} The method for predicting and receiving the
time of the beam spill is described in
Sec.~\ref{sec:fdsp-daq-design-beamtiming}.  Once that time is known to
the DAQ, a high level trigger can be issued to ensure that the necessary
full data can be saved from the buffer and saved as an event.

\paragraph{Astrophysical Triggers:} The Supernova Early Warning System
(SNEWS) is a coincidence network of neutrino experiments which are
individually sensitive to the burst of neutrinos which would be observed
from a core-collapse supernova somewhere in our galaxy.  While DUNE
should be sensitive to such a burst on its own able to contribute to the
coincidence network (Sec.~\ref{sec:fdsp-daq-sel}) via a tcp socket,
being able to save data based on other observations adds an additional
chance to not miss saving this rare and valuable data.  A SNEWS alert is
formed when two or more neutrino experiments report a potential
supernova signal within \SI{10}{s}.  The earliest time in the
coincidence is then sent via running a script on the SNEWS server at BNL
provided by the experiment wishing to receive the alert.  The latency
from neutrino burst is set by the response time of the second fastest
detector to report to SNEWS: this could be as short as seconds, but
might be tens of seconds.  At latencies larger than \SI{10}{s}, full
data might not be available, but compressed data might writeable.

There are other astrophysical triggers available for occurrences which
DUNE might not sensitive to individually, but could be either rarely or
if taken as an ensemble.  Gravitational wave triggers will be available
(details being worked out during the current LIGO shutdown), as will
high energy photon transients, most notably gamma ray bursts.  In fact,
cooperation between LIGO/VIRGO, the Gamma Ray Coordinates Network
(GCN)\footnote{Described in detail at
  \url{https://gcn.gsfc.nasa.gov/gcn_describe.html}}, and a number of
automated telescopes via network sockets on the time scale of seconds
enabled the discovery that ``short/hard'' gamma ray bursts are caused by
colliding neutron stars\cite{kilonova}.
